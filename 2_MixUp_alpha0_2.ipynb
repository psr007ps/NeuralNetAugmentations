{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_MixUp_alpha0.2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0uVvkIZNG3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MixupGenerator():\n",
        "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.shuffle = shuffle\n",
        "        self.sample_num = len(X_train)\n",
        "        self.datagen = datagen\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            indexes = self.__get_exploration_order()\n",
        "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
        "\n",
        "            for i in range(itr_num):\n",
        "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
        "                X, y = self.__data_generation(batch_ids)\n",
        "\n",
        "                yield X, y\n",
        "\n",
        "    def __get_exploration_order(self):\n",
        "        indexes = np.arange(self.sample_num)\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "        return indexes\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        _, h, w, c = self.X_train.shape\n",
        "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
        "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
        "        y_l = l.reshape(self.batch_size, 1)\n",
        "\n",
        "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
        "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "\n",
        "        if self.datagen:\n",
        "            for i in range(self.batch_size):\n",
        "                X[i] = self.datagen.random_transform(X[i])\n",
        "                X[i] = self.datagen.standardize(X[i])\n",
        "\n",
        "        if isinstance(self.y_train, list):\n",
        "            y = []\n",
        "\n",
        "            for y_train_ in self.y_train:\n",
        "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
        "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
        "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
        "        else:\n",
        "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
        "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
        "            y = y1 * y_l + y2 * (1 - y_l)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYboF7xC_Cmr",
        "colab_type": "code",
        "outputId": "4bd8de5a-7d74-4c79-8ff3-611b719e3d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 100\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "depth = 20\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv' % (depth)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "#take 10000 samples out of 50000 for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, X_test, y_train, Y_test = train_test_split(x_train, y_train, stratify=y_train, test_size=0.8)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        " \n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "csv_logger = CSVLogger('training.log')\n",
        "\n",
        "\n",
        "training_generator = MixupGenerator(x_train, y_train, batch_size=batch_size, alpha=0.2)()\n",
        "model.fit_generator(generator=training_generator,\n",
        "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        callbacks=[csv_logger]\n",
        "                        )\n",
        "\n",
        "             \n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (10000, 32, 32, 3)\n",
            "10000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (10000, 1)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   4160        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  8320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_12[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 64)   8256        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 128)    16512       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 128)    147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 128)    512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 128)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 256)    33024       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 256)    33024       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 256)    0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 128)    32896       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 128)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 256)    33024       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           add_5[0][0]                      \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 256)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 2.3172 - accuracy: 0.3390 - val_loss: 2.8523 - val_accuracy: 0.1424\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 215s 3s/step - loss: 2.0051 - accuracy: 0.4621 - val_loss: 3.0342 - val_accuracy: 0.1931\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 1.8566 - accuracy: 0.5201 - val_loss: 2.0976 - val_accuracy: 0.3833\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 221s 3s/step - loss: 1.7323 - accuracy: 0.5700 - val_loss: 1.8665 - val_accuracy: 0.4421\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 1.6269 - accuracy: 0.6128 - val_loss: 1.9072 - val_accuracy: 0.4357\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 223s 3s/step - loss: 1.5345 - accuracy: 0.6542 - val_loss: 1.7957 - val_accuracy: 0.5030\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 1.4308 - accuracy: 0.6962 - val_loss: 2.8762 - val_accuracy: 0.3041\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 231s 3s/step - loss: 1.3486 - accuracy: 0.7392 - val_loss: 1.8828 - val_accuracy: 0.5043\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 229s 3s/step - loss: 1.2674 - accuracy: 0.7729 - val_loss: 1.9404 - val_accuracy: 0.4880\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 1.2064 - accuracy: 0.7907 - val_loss: 1.9126 - val_accuracy: 0.4937\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 233s 3s/step - loss: 1.1255 - accuracy: 0.8252 - val_loss: 2.6721 - val_accuracy: 0.3642\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 232s 3s/step - loss: 1.1078 - accuracy: 0.8328 - val_loss: 1.9267 - val_accuracy: 0.4956\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 234s 3s/step - loss: 1.0556 - accuracy: 0.8633 - val_loss: 2.5109 - val_accuracy: 0.4411\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 230s 3s/step - loss: 1.0237 - accuracy: 0.8699 - val_loss: 1.8872 - val_accuracy: 0.4999\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 0.9985 - accuracy: 0.8793 - val_loss: 1.6528 - val_accuracy: 0.5633\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 237s 3s/step - loss: 0.9716 - accuracy: 0.8853 - val_loss: 3.4411 - val_accuracy: 0.3075\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 233s 3s/step - loss: 0.9534 - accuracy: 0.8952 - val_loss: 2.7446 - val_accuracy: 0.3945\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 237s 3s/step - loss: 0.9173 - accuracy: 0.9051 - val_loss: 1.9164 - val_accuracy: 0.4971\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 231s 3s/step - loss: 0.9097 - accuracy: 0.9082 - val_loss: 2.4403 - val_accuracy: 0.4305\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 232s 3s/step - loss: 0.9058 - accuracy: 0.9104 - val_loss: 2.3224 - val_accuracy: 0.4434\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 233s 3s/step - loss: 0.9002 - accuracy: 0.9100 - val_loss: 2.0887 - val_accuracy: 0.5082\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 230s 3s/step - loss: 0.8917 - accuracy: 0.9101 - val_loss: 2.4487 - val_accuracy: 0.4719\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 230s 3s/step - loss: 0.8709 - accuracy: 0.9098 - val_loss: 1.7540 - val_accuracy: 0.5352\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 237s 3s/step - loss: 0.8566 - accuracy: 0.9226 - val_loss: 1.9391 - val_accuracy: 0.4961\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 230s 3s/step - loss: 0.8628 - accuracy: 0.9166 - val_loss: 2.2586 - val_accuracy: 0.4551\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 233s 3s/step - loss: 0.8624 - accuracy: 0.9173 - val_loss: 1.7334 - val_accuracy: 0.5610\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 227s 3s/step - loss: 0.8544 - accuracy: 0.9245 - val_loss: 1.8954 - val_accuracy: 0.5026\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.8393 - accuracy: 0.9231 - val_loss: 1.8529 - val_accuracy: 0.5019\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 230s 3s/step - loss: 0.8461 - accuracy: 0.9165 - val_loss: 1.8133 - val_accuracy: 0.5457\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.8399 - accuracy: 0.9216 - val_loss: 1.8272 - val_accuracy: 0.5121\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.8297 - accuracy: 0.9261 - val_loss: 2.1750 - val_accuracy: 0.4626\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 232s 3s/step - loss: 0.8292 - accuracy: 0.9229 - val_loss: 2.3686 - val_accuracy: 0.4823\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 229s 3s/step - loss: 0.8263 - accuracy: 0.9302 - val_loss: 2.4813 - val_accuracy: 0.4503\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 235s 3s/step - loss: 0.8169 - accuracy: 0.9224 - val_loss: 1.7277 - val_accuracy: 0.5339\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 227s 3s/step - loss: 0.8222 - accuracy: 0.9225 - val_loss: 2.1876 - val_accuracy: 0.4774\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.8249 - accuracy: 0.9199 - val_loss: 2.1056 - val_accuracy: 0.4865\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.8042 - accuracy: 0.9273 - val_loss: 2.1297 - val_accuracy: 0.4685\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7965 - accuracy: 0.9305 - val_loss: 2.4127 - val_accuracy: 0.4650\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 219s 3s/step - loss: 0.7987 - accuracy: 0.9317 - val_loss: 2.0983 - val_accuracy: 0.4866\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.8037 - accuracy: 0.9251 - val_loss: 1.7788 - val_accuracy: 0.5213\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.8100 - accuracy: 0.9257 - val_loss: 2.2399 - val_accuracy: 0.4613\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.8076 - accuracy: 0.9209 - val_loss: 1.7217 - val_accuracy: 0.5568\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.7739 - accuracy: 0.9342 - val_loss: 2.0321 - val_accuracy: 0.5202\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 221s 3s/step - loss: 0.7834 - accuracy: 0.9311 - val_loss: 2.5299 - val_accuracy: 0.4350\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7911 - accuracy: 0.9291 - val_loss: 2.0637 - val_accuracy: 0.5136\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 218s 3s/step - loss: 0.7877 - accuracy: 0.9263 - val_loss: 1.8145 - val_accuracy: 0.5293\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 215s 3s/step - loss: 0.7845 - accuracy: 0.9315 - val_loss: 1.9474 - val_accuracy: 0.5055\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.7696 - accuracy: 0.9345 - val_loss: 1.7008 - val_accuracy: 0.5576\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 219s 3s/step - loss: 0.7630 - accuracy: 0.9382 - val_loss: 1.7452 - val_accuracy: 0.5194\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 221s 3s/step - loss: 0.7729 - accuracy: 0.9348 - val_loss: 2.6926 - val_accuracy: 0.4144\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 225s 3s/step - loss: 0.7651 - accuracy: 0.9309 - val_loss: 1.8706 - val_accuracy: 0.5190\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 218s 3s/step - loss: 0.7681 - accuracy: 0.9363 - val_loss: 1.8193 - val_accuracy: 0.5088\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 217s 3s/step - loss: 0.7596 - accuracy: 0.9354 - val_loss: 1.6280 - val_accuracy: 0.5534\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 223s 3s/step - loss: 0.7780 - accuracy: 0.9306 - val_loss: 1.7407 - val_accuracy: 0.5329\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7700 - accuracy: 0.9359 - val_loss: 1.7034 - val_accuracy: 0.5455\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.7502 - accuracy: 0.9351 - val_loss: 1.5856 - val_accuracy: 0.5749\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7689 - accuracy: 0.9304 - val_loss: 1.7488 - val_accuracy: 0.5490\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7587 - accuracy: 0.9297 - val_loss: 1.9003 - val_accuracy: 0.5304\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 225s 3s/step - loss: 0.7559 - accuracy: 0.9308 - val_loss: 2.0851 - val_accuracy: 0.5106\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7589 - accuracy: 0.9342 - val_loss: 1.7181 - val_accuracy: 0.5544\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 220s 3s/step - loss: 0.7588 - accuracy: 0.9262 - val_loss: 1.5528 - val_accuracy: 0.5729\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.7699 - accuracy: 0.9278 - val_loss: 2.0599 - val_accuracy: 0.4869\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.7435 - accuracy: 0.9421 - val_loss: 2.6695 - val_accuracy: 0.3984\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 0.7524 - accuracy: 0.9329 - val_loss: 2.3743 - val_accuracy: 0.4285\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 229s 3s/step - loss: 0.7454 - accuracy: 0.9341 - val_loss: 2.3138 - val_accuracy: 0.4097\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.7459 - accuracy: 0.9389 - val_loss: 2.1906 - val_accuracy: 0.4473\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 227s 3s/step - loss: 0.7296 - accuracy: 0.9394 - val_loss: 1.6131 - val_accuracy: 0.5504\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7434 - accuracy: 0.9338 - val_loss: 1.8272 - val_accuracy: 0.5179\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.7366 - accuracy: 0.9379 - val_loss: 1.8838 - val_accuracy: 0.5254\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7411 - accuracy: 0.9379 - val_loss: 1.9124 - val_accuracy: 0.4802\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.7438 - accuracy: 0.9343 - val_loss: 1.8457 - val_accuracy: 0.5210\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7425 - accuracy: 0.9369 - val_loss: 2.0123 - val_accuracy: 0.4631\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 216s 3s/step - loss: 0.7268 - accuracy: 0.9379 - val_loss: 1.7965 - val_accuracy: 0.5250\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 218s 3s/step - loss: 0.7217 - accuracy: 0.9403 - val_loss: 2.0986 - val_accuracy: 0.4913\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 216s 3s/step - loss: 0.7353 - accuracy: 0.9328 - val_loss: 1.6184 - val_accuracy: 0.5535\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 217s 3s/step - loss: 0.7320 - accuracy: 0.9364 - val_loss: 2.2154 - val_accuracy: 0.4849\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 220s 3s/step - loss: 0.7310 - accuracy: 0.9386 - val_loss: 1.7153 - val_accuracy: 0.5351\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 216s 3s/step - loss: 0.7150 - accuracy: 0.9406 - val_loss: 2.2530 - val_accuracy: 0.4886\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 221s 3s/step - loss: 0.7203 - accuracy: 0.9395 - val_loss: 1.8903 - val_accuracy: 0.5287\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.7306 - accuracy: 0.9374 - val_loss: 1.6274 - val_accuracy: 0.5644\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 223s 3s/step - loss: 0.7388 - accuracy: 0.9328 - val_loss: 2.2190 - val_accuracy: 0.4285\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 229s 3s/step - loss: 0.7185 - accuracy: 0.9410 - val_loss: 2.0686 - val_accuracy: 0.5004\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 223s 3s/step - loss: 0.7213 - accuracy: 0.9396 - val_loss: 2.4170 - val_accuracy: 0.4423\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 223s 3s/step - loss: 0.7201 - accuracy: 0.9384 - val_loss: 1.5774 - val_accuracy: 0.5547\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 225s 3s/step - loss: 0.7104 - accuracy: 0.9417 - val_loss: 1.5521 - val_accuracy: 0.5802\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 222s 3s/step - loss: 0.7193 - accuracy: 0.9361 - val_loss: 1.6694 - val_accuracy: 0.5543\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 0.7045 - accuracy: 0.9410 - val_loss: 1.8248 - val_accuracy: 0.5092\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 225s 3s/step - loss: 0.7127 - accuracy: 0.9389 - val_loss: 2.9192 - val_accuracy: 0.3642\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 224s 3s/step - loss: 0.7045 - accuracy: 0.9399 - val_loss: 1.7197 - val_accuracy: 0.5476\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 0.7167 - accuracy: 0.9379 - val_loss: 2.3478 - val_accuracy: 0.4373\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 223s 3s/step - loss: 0.7029 - accuracy: 0.9419 - val_loss: 1.8852 - val_accuracy: 0.5438\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.7078 - accuracy: 0.9397 - val_loss: 3.3414 - val_accuracy: 0.3764\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 230s 3s/step - loss: 0.6958 - accuracy: 0.9434 - val_loss: 2.6221 - val_accuracy: 0.4093\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 226s 3s/step - loss: 0.7128 - accuracy: 0.9376 - val_loss: 2.0827 - val_accuracy: 0.4646\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 225s 3s/step - loss: 0.7035 - accuracy: 0.9437 - val_loss: 1.5273 - val_accuracy: 0.5704\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 232s 3s/step - loss: 0.6926 - accuracy: 0.9454 - val_loss: 2.1468 - val_accuracy: 0.4746\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 0.6952 - accuracy: 0.9388 - val_loss: 2.2664 - val_accuracy: 0.4754\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 232s 3s/step - loss: 0.7035 - accuracy: 0.9405 - val_loss: 1.7906 - val_accuracy: 0.5374\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 227s 3s/step - loss: 0.7005 - accuracy: 0.9406 - val_loss: 1.6136 - val_accuracy: 0.5584\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 228s 3s/step - loss: 0.6900 - accuracy: 0.9434 - val_loss: 2.0489 - val_accuracy: 0.4887\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "Test loss: 2.048850222015381\n",
            "Test accuracy: 0.4887000024318695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rfO647_c1mZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}