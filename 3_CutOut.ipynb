{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_CutOut.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rfO647_c1mZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, v_l=0, v_h=255):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        \n",
        "        w = 8\n",
        "        mid_x = np.random.randint(0, img_w)\n",
        "        mid_y = np.random.randint(0, img_h)\n",
        "        \n",
        "        c = np.random.uniform(v_l, v_h)\n",
        "        input_img[mid_x - w : mid_x + w, mid_y - w : mid_y + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJDI33sfQZdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#from random_eraser import get_random_eraser\n",
        "\n",
        "cols, rows = 5, 4\n",
        "img_num = cols * rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-U9XnM4dflY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "0c42c71a-bf20-467d-bc37-503550072589"
      },
      "source": [
        "\n",
        "x = np.zeros((img_num, 64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "eraser = get_random_eraser()\n",
        "\n",
        "for i in range(img_num):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(eraser(x[i]), interpolation=\"nearest\")\n",
        "    plt.axis('off')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAADnCAYAAABMpd6dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAEmUlEQVR4nO3dMU7rWhRA0e8PHdN4JeOgpKdgIgyAcSBR0FMyDkqmQYnur58cwjebEGzWamNFV6fYOiExnsYY/wDwef8e+wAAayekAJGQAkRCChAJKUB0uu/FaZpW/ZX+GGP66vc0k93MZc5M5rY6ExspQCSkAJGQAkR7/0YKsDUnJyeLrn97e/vwGhspQCSkAJGP9mzO8/PzouvPz88PdBJ+CxspQCSkANGHH+3v7+8XveH19fWnDwOwRjZSgEhIASIhBYiEFCASUoDID/KBX+X/3Du/lI0UIBJSgMhHezbHvfN8NxspQCSkANE0xvsP9dvqE/8KM9nNXObMZG6rM7GRAkRCChAJKUAkpACRkAJEQgoQCSlAJKQAkZACREIKEAkpQCSkAJGQAkRCChAJKUAkpACRkAJEQgoQCSlAJKQAkZACREIKEAkpQCSkAJGQAkTTGOPYZwBYNRspQCSkAJGQAkRCChAJKUAkpACRkAJEQgoQne57cZqmVf9af4wxffV7mslu5jJnJnNbnYmNFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUINr7FFFgnc7OzhZd//r6eqCT/A42UoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYjcaw8b5N7572UjBYiEFCASUoBISAEiXzYBq/Hw8LDo+qurqwOd5G82UoBISAEiIQWIhBQgElKASEgBIiEFiKYxxrHPALBqNlKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBo73/In6Zp1bc9jTGmr35PM9nNXObMZG6rM7GRAkRCChAJKUAkpACRkAJEQgoQCSlAJKQA0d4f5P8kT09Pi66/uLg40EkA/mYjBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgGga4/1nUW31QVWFmexmLnNmMrfVmdhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIAaJpjHHsMwCsmo0UIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgOt334jRNq77taYwxffV7mslu5jJnJnNbnYmNFCASUoBISAEiIQWI9n7ZBHBot7e3i66/ubk50Ek+z0YKEAkpQOSj/Ybc3d0d+wjwK9lIASIhBYiEFCASUoBISAEi39rDCry8vCy6/s+fPwc6CbvYSAEiIQWIfLQHjuon3ju/lI0UIBJSgGga4/1HqGz1+SqFmexmLnNfOZNjfGv/02dyDJ7ZBHAgQgoQCSlAJKQAkZACRL61X8hMdjOXOTOZ2+pM3NnEl3t8fFx0/eXl5YFOAt/DR3uASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWI/NOShcxkN3OZM5O5rc7ERgoQCSlAJKQAkZACREIKEAkpQCSkANHe35EC8DEbKUAkpACRkAJEQgoQCSlAJKQA0X9LlPtze4n02wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVbjtvRtnF_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f59d76b6-e64b-4a72-f7a8-fe0ab949fe1b"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 100\n",
        "num_classes = 10\n",
        "pixel_level = False\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "depth = 20\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv' % (depth)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "#take 10000 samples out of 50000 for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, X_test, y_train, Y_test = train_test_split(x_train, y_train, stratify=y_train, test_size=0.8)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        " \n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "csv_logger = CSVLogger('training.log')\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1))\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=epochs, verbose=1, workers=4,\n",
        "                    callbacks=[csv_logger])\n",
        "             \n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "x_train shape: (10000, 32, 32, 3)\n",
            "10000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (10000, 1)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   4160        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  8320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_12[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 64)   8256        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 128)    16512       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 128)    147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 128)    512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 128)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 256)    33024       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 256)    33024       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 256)    0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 128)    32896       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 128)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 256)    33024       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           add_5[0][0]                      \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 256)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v\n",
            "Epoch 1/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 2.2554 - accuracy: 0.3289 - val_loss: 2.8313 - val_accuracy: 0.1552\n",
            "Epoch 2/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 1.9325 - accuracy: 0.4410 - val_loss: 2.5726 - val_accuracy: 0.2531\n",
            "Epoch 3/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 1.7419 - accuracy: 0.5041 - val_loss: 2.6236 - val_accuracy: 0.2609\n",
            "Epoch 4/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 1.6184 - accuracy: 0.5508 - val_loss: 2.0738 - val_accuracy: 0.3940\n",
            "Epoch 5/100\n",
            "79/79 [==============================] - 261s 3s/step - loss: 1.4802 - accuracy: 0.5947 - val_loss: 1.9678 - val_accuracy: 0.4582\n",
            "Epoch 6/100\n",
            "79/79 [==============================] - 267s 3s/step - loss: 1.3475 - accuracy: 0.6465 - val_loss: 2.1783 - val_accuracy: 0.4077\n",
            "Epoch 7/100\n",
            "79/79 [==============================] - 264s 3s/step - loss: 1.2518 - accuracy: 0.6768 - val_loss: 2.5732 - val_accuracy: 0.3715\n",
            "Epoch 8/100\n",
            "79/79 [==============================] - 273s 3s/step - loss: 1.1236 - accuracy: 0.7301 - val_loss: 2.3487 - val_accuracy: 0.4735\n",
            "Epoch 9/100\n",
            "79/79 [==============================] - 269s 3s/step - loss: 1.0240 - accuracy: 0.7637 - val_loss: 2.3219 - val_accuracy: 0.4297\n",
            "Epoch 10/100\n",
            "79/79 [==============================] - 271s 3s/step - loss: 0.9907 - accuracy: 0.7748 - val_loss: 2.0476 - val_accuracy: 0.4834\n",
            "Epoch 11/100\n",
            "79/79 [==============================] - 264s 3s/step - loss: 0.8802 - accuracy: 0.8198 - val_loss: 2.2596 - val_accuracy: 0.4912\n",
            "Epoch 12/100\n",
            "79/79 [==============================] - 253s 3s/step - loss: 0.8080 - accuracy: 0.8437 - val_loss: 3.9558 - val_accuracy: 0.3502\n",
            "Epoch 13/100\n",
            "79/79 [==============================] - 255s 3s/step - loss: 0.7078 - accuracy: 0.8800 - val_loss: 3.4923 - val_accuracy: 0.3763\n",
            "Epoch 14/100\n",
            "79/79 [==============================] - 252s 3s/step - loss: 0.7280 - accuracy: 0.8734 - val_loss: 3.2478 - val_accuracy: 0.4216\n",
            "Epoch 15/100\n",
            "79/79 [==============================] - 272s 3s/step - loss: 0.6746 - accuracy: 0.8935 - val_loss: 3.3799 - val_accuracy: 0.4261\n",
            "Epoch 16/100\n",
            "79/79 [==============================] - 268s 3s/step - loss: 0.6803 - accuracy: 0.8851 - val_loss: 2.7300 - val_accuracy: 0.4537\n",
            "Epoch 17/100\n",
            "79/79 [==============================] - 267s 3s/step - loss: 0.5970 - accuracy: 0.9187 - val_loss: 2.6463 - val_accuracy: 0.4763\n",
            "Epoch 18/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.5620 - accuracy: 0.9317 - val_loss: 2.8686 - val_accuracy: 0.4743\n",
            "Epoch 19/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.6246 - accuracy: 0.9021 - val_loss: 2.7887 - val_accuracy: 0.4759\n",
            "Epoch 20/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.5491 - accuracy: 0.9293 - val_loss: 2.9100 - val_accuracy: 0.5073\n",
            "Epoch 21/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.5366 - accuracy: 0.9335 - val_loss: 3.2965 - val_accuracy: 0.4600\n",
            "Epoch 22/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.5507 - accuracy: 0.9285 - val_loss: 2.3958 - val_accuracy: 0.5155\n",
            "Epoch 23/100\n",
            "79/79 [==============================] - 234s 3s/step - loss: 0.5198 - accuracy: 0.9397 - val_loss: 3.6641 - val_accuracy: 0.4170\n",
            "Epoch 24/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.5277 - accuracy: 0.9336 - val_loss: 2.9427 - val_accuracy: 0.4998\n",
            "Epoch 25/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.5366 - accuracy: 0.9322 - val_loss: 3.7223 - val_accuracy: 0.4693\n",
            "Epoch 26/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.5711 - accuracy: 0.9188 - val_loss: 2.4799 - val_accuracy: 0.5127\n",
            "Epoch 27/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.5190 - accuracy: 0.9352 - val_loss: 2.6925 - val_accuracy: 0.5213\n",
            "Epoch 28/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.5029 - accuracy: 0.9420 - val_loss: 3.1137 - val_accuracy: 0.4615\n",
            "Epoch 29/100\n",
            "79/79 [==============================] - 234s 3s/step - loss: 0.5020 - accuracy: 0.9411 - val_loss: 2.7584 - val_accuracy: 0.4959\n",
            "Epoch 30/100\n",
            "79/79 [==============================] - 235s 3s/step - loss: 0.4892 - accuracy: 0.9450 - val_loss: 2.6630 - val_accuracy: 0.5266\n",
            "Epoch 31/100\n",
            "79/79 [==============================] - 241s 3s/step - loss: 0.4885 - accuracy: 0.9463 - val_loss: 4.9894 - val_accuracy: 0.3539\n",
            "Epoch 32/100\n",
            "79/79 [==============================] - 236s 3s/step - loss: 0.4704 - accuracy: 0.9509 - val_loss: 3.2557 - val_accuracy: 0.4825\n",
            "Epoch 33/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.5160 - accuracy: 0.9341 - val_loss: 3.0804 - val_accuracy: 0.5092\n",
            "Epoch 34/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.4703 - accuracy: 0.9493 - val_loss: 4.4885 - val_accuracy: 0.4429\n",
            "Epoch 35/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4497 - accuracy: 0.9586 - val_loss: 4.4127 - val_accuracy: 0.4505\n",
            "Epoch 36/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.4729 - accuracy: 0.9485 - val_loss: 3.4950 - val_accuracy: 0.4495\n",
            "Epoch 37/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4890 - accuracy: 0.9389 - val_loss: 4.4092 - val_accuracy: 0.4333\n",
            "Epoch 38/100\n",
            "79/79 [==============================] - 235s 3s/step - loss: 0.5156 - accuracy: 0.9316 - val_loss: 2.9500 - val_accuracy: 0.5313\n",
            "Epoch 39/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.5314 - accuracy: 0.9310 - val_loss: 4.8771 - val_accuracy: 0.3872\n",
            "Epoch 40/100\n",
            "79/79 [==============================] - 236s 3s/step - loss: 0.4790 - accuracy: 0.9449 - val_loss: 3.5794 - val_accuracy: 0.4405\n",
            "Epoch 41/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.4589 - accuracy: 0.9548 - val_loss: 4.6547 - val_accuracy: 0.3676\n",
            "Epoch 42/100\n",
            "79/79 [==============================] - 227s 3s/step - loss: 0.4943 - accuracy: 0.9398 - val_loss: 4.0247 - val_accuracy: 0.4481\n",
            "Epoch 43/100\n",
            "79/79 [==============================] - 225s 3s/step - loss: 0.4783 - accuracy: 0.9432 - val_loss: 3.0335 - val_accuracy: 0.4955\n",
            "Epoch 44/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4827 - accuracy: 0.9442 - val_loss: 3.8820 - val_accuracy: 0.3902\n",
            "Epoch 45/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.4316 - accuracy: 0.9625 - val_loss: 2.7972 - val_accuracy: 0.5155\n",
            "Epoch 46/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.4623 - accuracy: 0.9526 - val_loss: 3.8700 - val_accuracy: 0.4587\n",
            "Epoch 47/100\n",
            "79/79 [==============================] - 227s 3s/step - loss: 0.5920 - accuracy: 0.9066 - val_loss: 2.9275 - val_accuracy: 0.5081\n",
            "Epoch 48/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4550 - accuracy: 0.9527 - val_loss: 3.7001 - val_accuracy: 0.4504\n",
            "Epoch 49/100\n",
            "79/79 [==============================] - 235s 3s/step - loss: 0.4627 - accuracy: 0.9494 - val_loss: 3.0919 - val_accuracy: 0.5132\n",
            "Epoch 50/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.4675 - accuracy: 0.9490 - val_loss: 3.6827 - val_accuracy: 0.4626\n",
            "Epoch 51/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.4295 - accuracy: 0.9611 - val_loss: 3.1652 - val_accuracy: 0.4900\n",
            "Epoch 52/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.4203 - accuracy: 0.9663 - val_loss: 3.4013 - val_accuracy: 0.4864\n",
            "Epoch 53/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4796 - accuracy: 0.9454 - val_loss: 3.7521 - val_accuracy: 0.4779\n",
            "Epoch 54/100\n",
            "79/79 [==============================] - 238s 3s/step - loss: 0.4319 - accuracy: 0.9597 - val_loss: 3.1420 - val_accuracy: 0.5027\n",
            "Epoch 55/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.4207 - accuracy: 0.9615 - val_loss: 4.3921 - val_accuracy: 0.4562\n",
            "Epoch 56/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.4151 - accuracy: 0.9649 - val_loss: 2.8178 - val_accuracy: 0.5195\n",
            "Epoch 57/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.4366 - accuracy: 0.9568 - val_loss: 3.4677 - val_accuracy: 0.4659\n",
            "Epoch 58/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4364 - accuracy: 0.9572 - val_loss: 3.5885 - val_accuracy: 0.4627\n",
            "Epoch 59/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4416 - accuracy: 0.9542 - val_loss: 4.3501 - val_accuracy: 0.4748\n",
            "Epoch 60/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4494 - accuracy: 0.9540 - val_loss: 4.8578 - val_accuracy: 0.3965\n",
            "Epoch 61/100\n",
            "79/79 [==============================] - 233s 3s/step - loss: 0.4781 - accuracy: 0.9448 - val_loss: 3.7145 - val_accuracy: 0.4667\n",
            "Epoch 62/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4327 - accuracy: 0.9571 - val_loss: 3.7185 - val_accuracy: 0.4129\n",
            "Epoch 63/100\n",
            "79/79 [==============================] - 232s 3s/step - loss: 0.4362 - accuracy: 0.9584 - val_loss: 2.6409 - val_accuracy: 0.5334\n",
            "Epoch 64/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4444 - accuracy: 0.9516 - val_loss: 5.6645 - val_accuracy: 0.3895\n",
            "Epoch 65/100\n",
            "79/79 [==============================] - 224s 3s/step - loss: 0.4724 - accuracy: 0.9427 - val_loss: 5.0569 - val_accuracy: 0.3949\n",
            "Epoch 66/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.4342 - accuracy: 0.9565 - val_loss: 3.1202 - val_accuracy: 0.4903\n",
            "Epoch 67/100\n",
            "79/79 [==============================] - 226s 3s/step - loss: 0.4187 - accuracy: 0.9613 - val_loss: 2.5557 - val_accuracy: 0.5546\n",
            "Epoch 68/100\n",
            "79/79 [==============================] - 226s 3s/step - loss: 0.4136 - accuracy: 0.9648 - val_loss: 3.5870 - val_accuracy: 0.4735\n",
            "Epoch 69/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.4371 - accuracy: 0.9532 - val_loss: 5.6050 - val_accuracy: 0.3836\n",
            "Epoch 70/100\n",
            "79/79 [==============================] - 224s 3s/step - loss: 0.3960 - accuracy: 0.9692 - val_loss: 4.0463 - val_accuracy: 0.4370\n",
            "Epoch 71/100\n",
            "79/79 [==============================] - 224s 3s/step - loss: 0.4770 - accuracy: 0.9424 - val_loss: 3.7217 - val_accuracy: 0.4744\n",
            "Epoch 72/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4473 - accuracy: 0.9504 - val_loss: 2.3116 - val_accuracy: 0.5612\n",
            "Epoch 73/100\n",
            "79/79 [==============================] - 223s 3s/step - loss: 0.4573 - accuracy: 0.9492 - val_loss: 3.1142 - val_accuracy: 0.5134\n",
            "Epoch 74/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4132 - accuracy: 0.9604 - val_loss: 3.5830 - val_accuracy: 0.4545\n",
            "Epoch 75/100\n",
            "79/79 [==============================] - 226s 3s/step - loss: 0.3977 - accuracy: 0.9683 - val_loss: 2.6469 - val_accuracy: 0.5407\n",
            "Epoch 76/100\n",
            "79/79 [==============================] - 225s 3s/step - loss: 0.3952 - accuracy: 0.9696 - val_loss: 2.8870 - val_accuracy: 0.5122\n",
            "Epoch 77/100\n",
            "79/79 [==============================] - 233s 3s/step - loss: 0.4103 - accuracy: 0.9644 - val_loss: 3.6140 - val_accuracy: 0.4553\n",
            "Epoch 78/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.4268 - accuracy: 0.9562 - val_loss: 3.9477 - val_accuracy: 0.4640\n",
            "Epoch 79/100\n",
            "79/79 [==============================] - 230s 3s/step - loss: 0.4156 - accuracy: 0.9607 - val_loss: 3.7230 - val_accuracy: 0.5216\n",
            "Epoch 80/100\n",
            "79/79 [==============================] - 234s 3s/step - loss: 0.4403 - accuracy: 0.9521 - val_loss: 3.7213 - val_accuracy: 0.5052\n",
            "Epoch 81/100\n",
            "79/79 [==============================] - 226s 3s/step - loss: 0.4402 - accuracy: 0.9537 - val_loss: 3.6910 - val_accuracy: 0.4797\n",
            "Epoch 82/100\n",
            "79/79 [==============================] - 234s 3s/step - loss: 0.4030 - accuracy: 0.9638 - val_loss: 4.0634 - val_accuracy: 0.4694\n",
            "Epoch 83/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4204 - accuracy: 0.9560 - val_loss: 2.8989 - val_accuracy: 0.5513\n",
            "Epoch 84/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4157 - accuracy: 0.9602 - val_loss: 6.0361 - val_accuracy: 0.3462\n",
            "Epoch 85/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4080 - accuracy: 0.9619 - val_loss: 3.3770 - val_accuracy: 0.4663\n",
            "Epoch 86/100\n",
            "79/79 [==============================] - 226s 3s/step - loss: 0.4778 - accuracy: 0.9396 - val_loss: 3.2585 - val_accuracy: 0.4905\n",
            "Epoch 87/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.4103 - accuracy: 0.9621 - val_loss: 3.2342 - val_accuracy: 0.4977\n",
            "Epoch 88/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.4222 - accuracy: 0.9587 - val_loss: 2.6961 - val_accuracy: 0.5365\n",
            "Epoch 89/100\n",
            "79/79 [==============================] - 227s 3s/step - loss: 0.4260 - accuracy: 0.9566 - val_loss: 3.3047 - val_accuracy: 0.4940\n",
            "Epoch 90/100\n",
            "79/79 [==============================] - 231s 3s/step - loss: 0.4043 - accuracy: 0.9631 - val_loss: 2.7329 - val_accuracy: 0.5394\n",
            "Epoch 91/100\n",
            "79/79 [==============================] - 227s 3s/step - loss: 0.4011 - accuracy: 0.9651 - val_loss: 3.6973 - val_accuracy: 0.4737\n",
            "Epoch 92/100\n",
            "79/79 [==============================] - 227s 3s/step - loss: 0.4576 - accuracy: 0.9497 - val_loss: 4.9894 - val_accuracy: 0.4777\n",
            "Epoch 93/100\n",
            "79/79 [==============================] - 234s 3s/step - loss: 0.3870 - accuracy: 0.9706 - val_loss: 2.5230 - val_accuracy: 0.5487\n",
            "Epoch 94/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.3823 - accuracy: 0.9694 - val_loss: 4.6133 - val_accuracy: 0.4199\n",
            "Epoch 95/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4244 - accuracy: 0.9558 - val_loss: 3.3025 - val_accuracy: 0.5072\n",
            "Epoch 96/100\n",
            "79/79 [==============================] - 234s 3s/step - loss: 0.3920 - accuracy: 0.9658 - val_loss: 2.8673 - val_accuracy: 0.5311\n",
            "Epoch 97/100\n",
            "79/79 [==============================] - 227s 3s/step - loss: 0.3712 - accuracy: 0.9726 - val_loss: 3.6652 - val_accuracy: 0.4733\n",
            "Epoch 98/100\n",
            "79/79 [==============================] - 235s 3s/step - loss: 0.3788 - accuracy: 0.9704 - val_loss: 3.7416 - val_accuracy: 0.4654\n",
            "Epoch 99/100\n",
            "79/79 [==============================] - 229s 3s/step - loss: 0.4595 - accuracy: 0.9440 - val_loss: 4.5566 - val_accuracy: 0.4542\n",
            "Epoch 100/100\n",
            "79/79 [==============================] - 228s 3s/step - loss: 0.3956 - accuracy: 0.9661 - val_loss: 5.4359 - val_accuracy: 0.4196\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "Test loss: 5.43591508026123\n",
            "Test accuracy: 0.4196000099182129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFXODoGRrSbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}